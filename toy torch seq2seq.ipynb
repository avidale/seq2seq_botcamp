{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2018/06/taming-lstms-variable-sized-mini-batches-pytorch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_1_x = ['is', 'it', 'too', 'late', 'now', 'say', 'sorry']\n",
    "sent_1_y = ['VB', 'PRP', 'RB', 'RB', 'RB', 'VB', 'JJ']\n",
    "\n",
    "sent_2_x = ['ooh', 'ooh']\n",
    "sent_2_y = ['NNP', 'NNP']\n",
    "\n",
    "sent_3_x = ['sorry', 'yeah']\n",
    "sent_3_y = ['JJ', 'NNP']\n",
    "\n",
    "X = [sent_1_x, sent_2_x, sent_3_x]\n",
    "Y = [sent_1_y, sent_2_y, sent_3_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map sentences to vocab\n",
    "vocab = {'': 0, 'is': 1, 'it': 2, 'too': 3, 'late': 4, 'now': 5, 'say': 6, 'sorry': 7, 'ooh': 8, 'yeah': 9} \n",
    "\n",
    "# fancy nested list comprehension\n",
    "X =  [[vocab[word] for word in sentence] for sentence in X]\n",
    "\n",
    "# X now looks like:  \n",
    "# [[1, 2, 3, 4, 5, 6, 7], [8, 8], [7, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = {'': 0, 'VB': 1, 'PRP': 2, 'RB': 3, 'JJ': 4, 'NNP': 5}\n",
    "\n",
    "# fancy nested list comprehension\n",
    "Y =  [[tags[tag] for tag in sentence] for sentence in Y]\n",
    "\n",
    "# Y now looks like:\n",
    "# [[1, 2, 3, 3, 3, 1, 4], [5, 5], [4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 3., 4., 5., 6.],\n",
       "       [7., 7., 0., 0., 0., 0., 0.],\n",
       "       [6., 8., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = [[0, 1, 2, 3, 4, 5, 6], \n",
    "    [7, 7], \n",
    "    [6, 8]]\n",
    "\n",
    "# get the length of each sentence\n",
    "X_lengths = [len(sentence) for sentence in X]\n",
    "\n",
    "# create an empty matrix with padding tokens\n",
    "pad_token = vocab['']\n",
    "longest_sent = max(X_lengths)\n",
    "batch_size = len(X)\n",
    "padded_X = np.ones((batch_size, longest_sent)) * pad_token\n",
    "\n",
    "# copy over the actual sequences\n",
    "for i, x_len in enumerate(X_lengths):\n",
    "    sequence = X[i]\n",
    "    padded_X[i, 0:x_len] = sequence[:x_len]\n",
    "\n",
    "padded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 3., 3., 1., 4.],\n",
       "       [5., 5., 0., 0., 0., 0., 0.],\n",
       "       [4., 5., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Y = [[1, 2, 3, 3, 3, 1, 4], \n",
    "    [5, 5], \n",
    "    [4, 5]]\n",
    "\n",
    "# get the length of each sentence\n",
    "Y_lengths = [len(sentence) for sentence in Y]\n",
    "\n",
    "# create an empty matrix with padding tokens\n",
    "pad_token = tags['']\n",
    "longest_sent = max(Y_lengths)\n",
    "batch_size = len(Y)\n",
    "padded_Y = np.ones((batch_size, longest_sent)) * pad_token\n",
    "\n",
    "# copy over the actual sequences\n",
    "for i, y_len in enumerate(Y_lengths):\n",
    "    sequence = Y[i]\n",
    "    padded_Y[i, 0:y_len] = sequence[:y_len]\n",
    "\n",
    "padded_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\"\"\"\n",
    "Blog post:\n",
    "Taming LSTMs: Variable-sized mini-batches and why PyTorch is good for your health:\n",
    "https://medium.com/@_willfalcon/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class BieberLSTM(nn.Module):\n",
    "    def __init__(self, nb_layers, nb_lstm_units=100, embedding_dim=3, batch_size=3):\n",
    "        self.vocab = {'<PAD>': 0, 'is': 1, 'it': 2, 'too': 3, 'late': 4, 'now': 5, 'say': 6, 'sorry': 7, 'ooh': 8,\n",
    "                      'yeah': 9}\n",
    "        self.tags = {'<PAD>': 0, 'VB': 1, 'PRP': 2, 'RB': 3, 'JJ': 4, 'NNP': 5}\n",
    "\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # don't count the padding tag for the classifier output\n",
    "        self.nb_tags = len(self.tags) - 1\n",
    "\n",
    "        # when the model is bidirectional we double the output dimension\n",
    "        self.lstm\n",
    "\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        # build embedding layer first\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        # whenever the embedding sees the padding index it'll make the whole vector zeros\n",
    "        padding_idx = self.vocab['<PAD>']\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "        # design LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # output layer which projects back to tag space\n",
    "        self.hidden_to_tag = nn.Linear(self.nb_lstm_units, self.nb_tags)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(self.hparams.nb_lstm_layers, self.batch_size, self.nb_lstm_units)\n",
    "        hidden_b = torch.randn(self.hparams.nb_lstm_layers, self.batch_size, self.nb_lstm_units)\n",
    "\n",
    "        if self.hparams.on_gpu:\n",
    "            hidden_a = hidden_a.cuda()\n",
    "            hidden_b = hidden_b.cuda()\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        # ---------------------\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len, 1) -> (batch_size, seq_len, embedding_dim)\n",
    "        X = self.word_embedding(X)\n",
    "\n",
    "        # ---------------------\n",
    "        # 2. Run through RNN\n",
    "        # TRICK 2 ********************************\n",
    "        # Dim transformation: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, nb_lstm_units)\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        X = torch.nn.utils.rnn.pack_padded_sequence(x, X_lengths, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "\n",
    "        # undo the packing operation\n",
    "        X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        # ---------------------\n",
    "        # 3. Project to tag space\n",
    "        # Dim transformation: (batch_size, seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_lstm_units)\n",
    "\n",
    "        # this one is a bit tricky as well. First we need to reshape the data so it goes into the linear layer\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        # run through actual linear layer\n",
    "        X = self.hidden_to_tag(X)\n",
    "\n",
    "        # ---------------------\n",
    "        # 4. Create softmax activations bc we're doing classification\n",
    "        # Dim transformation: (batch_size * seq_len, nb_lstm_units) -> (batch_size, seq_len, nb_tags)\n",
    "        X = F.log_softmax(X, dim=1)\n",
    "\n",
    "        # I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\n",
    "        X = X.view(batch_size, seq_len, self.nb_tags)\n",
    "\n",
    "        Y_hat = X\n",
    "        return Y_hat\n",
    "\n",
    "    def loss(self, Y_hat, Y, X_lengths):\n",
    "        # TRICK 3 ********************************\n",
    "        # before we calculate the negative log likelihood, we need to mask out the activations\n",
    "        # this means we don't want to take into account padded items in the output vector\n",
    "        # simplest way to think about this is to flatten ALL sequences into a REALLY long sequence\n",
    "        # and calculate the loss on that.\n",
    "\n",
    "        # flatten all the labels\n",
    "        Y = Y.view(-1)\n",
    "\n",
    "        # flatten all predictions\n",
    "        Y_hat = Y_hat.view(-1, self.nb_tags)\n",
    "\n",
    "        # create a mask by filtering out all tokens that ARE NOT the padding token\n",
    "        tag_pad_token = self.tags['<PAD>']\n",
    "        mask = (Y > tag_pad_token).float()\n",
    "\n",
    "        # count how many tokens we have\n",
    "        nb_tokens = int(torch.sum(mask).data[0])\n",
    "\n",
    "        # pick the values for the label and zero out the rest with the mask\n",
    "        Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask\n",
    "\n",
    "        # compute cross entropy loss which ignores all <PAD> tokens\n",
    "        ce_loss = -torch.sum(Y_hat) / nb_tokens\n",
    "\n",
    "        return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
